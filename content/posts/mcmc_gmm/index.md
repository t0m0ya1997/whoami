---
title: MCMCによる混合ガウスモデル（GMM）のベイズ推論
date: 2022-02-6
category: [Research-ish]
tags: [Bayesian Estimation, Mixture Model, Machine Learning, MCMC]
description: "この記事では，混合モデルのMCMCによるベイズ推論を紹介します．"
image: ""
math: true
draft: true
---

### そもそも
混合ガウスモデル（**GMM**：Gaussian Mixture Model）とは何でしょう．
はじめに直感的な紹介をします．

以下のようなデータが与えられたとしましょう．

![hoge](hoge)

「なんか３つの塊がある」といった印象を受けますね．
これらのデータ点には，所属を表すラベルがついていません．
しかしながら，**あるルール**に従ってうまく分類することができれば，以下のようにできそうですよね．

![hoge](hoge)

このように，**正解となるラベルが与えられていない場合にデータの傾向や特徴を学習すること**（また，それらを使って分類を行うこと）を「<mark>教師なし学習</mark>」と言います．
GMMも教師なし学習の１つで，上にあげたようにデータを分類することに使われたりします．

先ほども述べた**あるルール**というのを決めるというのが，機械学習モデルを決めることに他なりません．
今回は，GMMというルールに基づいてデータを分類すると上のような結果が得られますよ，というお話です．

GMMとは混合ガウスモデル（GMM：Gaussian Mixture Model），すなわち，いくつかのガウス分布の混合モデルです．
みなさんがよく知る単一のガウス分布は以下の形状じゃないでしょうか．

![hoge](hoge)

このガウス分布に従って発生するデータというのは，ガウス分布の平均値付近に集中するはずです．
しかし，データがいくつかの混合から発生している場合，１つのガウス分布から発生しているとは考えにくそうです．
そこで，こんな感じのガウス分布を混合させたものを考えるわけです．

![hoge](hoge)

こうすると，それぞれのガウス分布の平均値付近からデータが発生すると考えられます．
すなわち，いくつかの混合からなるデータにも対応できるようになります．

この例は1次元で説明しましたが，多次元空間を考えることもできます．
この記事では，2次元データに対してGMMというルールを定めてみます．

### GMM の定式化
混合ガウスモデル(Gaussian Mixture Model：GMM)の定式化を行います．
本記事では，2次元のGMMを考えます．
与えられた $n$ 個の $2$ 次元データ $x^n=(\bm x_1,\bm x_2,\cdots,\bm x_n),~\bm x_i\in\mathbb R^2,~(i=1,2,\cdots,n)$ が，以下の $K$ 個のガウス分布の混合により生成されたと仮定するのがGMMです．

$$
    p(x^n|\pi^K,\mu^K,\Sigma^K)=\prod_{i=1}^n\left(\sum_{k=1}^K\pi_k\mathcal N(\bm x_i|\bm\mu_k,\Sigma_k^{-1})\right)
    \tag{1}
$$

ここで，各パラメータの説明をします．
$\pi^K=(\pi_1,\pi_2,\cdots,\pi_K)$ は**混合比**です． 
$\pi_k$ は，$k$ 番目の混合における混合比を表し，データ全体の中で $k$ 番目の混合が占める相対的な比を表します．
よって，

$$
    \sum_{k=1}^K\pi_k = 1
    \tag{2}
$$

が成り立ちます．

$\mu^K=(\bm\mu_1,\bm\mu_2,\cdots,\bm\mu_K)$ は各混合におけるガウス分布の**平均**を表します．
「各混合の中心」のようなものです．
また，$\Sigma^K=(\Sigma_1,\Sigma_2,\cdots,\Sigma_K)$ は各混合におけるガウス分布の**分散・共分散行列**を表します．
「各混合でのデータの広がり方」に相当します．

GMMの定義は以上なのですが，よりモデルとして扱いやすいように，**潜在変数**を導入します．
人によっては「**ラベル変数**」と呼ぶ方がわかりやすいかもしれません．
ここでの潜在変数は，ある1つのデータ点が「**どの混合に属するか**」を明示的に表現します．
潜在変数を $l^n = (\bm l_1,\bm l_2,\cdots,\bm l_n)$ と表します．
ここで，ある $i$ 番目のデータ点 $\bm x_i$ に対応する潜在変数 $\bm l_i$ は，one-hotなベクトル，すなわち，

$$
    \bm l_i \in\\{0,1\\}^K,||\bm l_i||=1
    \tag{3}
$$

を満たすようなベクトルとします．
この潜在変数を用いると，式(1)の生成モデルは，

$$
    p(x^n,l^n|\pi^K,\mu^K,\Sigma^K) = \prod_{i=1}^n\prod_{k=1}^K\pi_k^{l_{i,k}}\mathcal N(\bm x_i|\bm\mu_k,\Sigma_k^{-1})^{l_{i,k}}
    \tag{4}
$$

と書き直すことができます．
ここで，潜在変数 $l^n$ について周辺化すると式(1)となることも確認できます．

以上より，GMMのデータの発生過程（確率モデル）を以下のように仮定します．


{{<notice note データ生成の流れ>}}
1. 混合数 $K$ が決定する．
2. 各混合における混合比 $\pi^K$，平均 $\mu^K$，分散・共分散行列 $\Sigma^K$ がそれぞれの事前分布 $p(\pi|K),p(\mu|K),p(\Sigma|K)$ より発生する．
3. 混合比 $\pi^K$ に基づいて，潜在変数 $l^n$ が発生する．
4. $i$ 番目のデータ点について，式(4)に基づいて，$\bm x_i$ が発生する $(i=1,2,\cdots,n)$．
{{</notice>}}

簡単のために，本記事では**各混合の分散・共分散行列は単位行列である** $^1$ という制約を課します．
これは，「**データは属する混合の中心から等方的に広がり生成される**」ということになります．
定数値とすることで，$\Sigma^K$ は推定するパラメータから除去されます．
また，混合数 $K$ の値は $K=3$ と事前に決めておきます $^{2, 3}$ ．

確率モデルのパラメータの依存関係を表現するグラフィカルモデルで表すと，以下のようになります．

{{<notice warning 注釈>}}
1. 本記事では，分散・共分散行列は対角行列であることを仮定しましたが，これは今回紹介するLabel Switching を説明する上で，そこまで重要ではないため簡略化しました．
一般の場合にはこれらも推定する場合が多いです．

2. 混合数についても，固定値として扱いました．
「適当に決めた混合数で推定して良いのか？」という疑問が湧き上がると思います．
その疑問は至極真っ当なものです．
本来は，混合数 $K$ をモデルのハイパーパラメータとして扱い，**モデル選択**により最適なモデル（混合数）を決定することが多いです．

3. さらなる注意点なのですが，**BIC**(ベイズ情報量規準)や**AIC**（赤池情報量規準）などを評価しモデル選択を行うと言う話を聞いたことがあるかもしれません．
これらは，パラメータのベイズ事後分布の漸近正規性が成り立つときに正しい値を示します．
一方で，この記事で扱う混合モデルはパラメータの事後分布に必ずしも漸近正規性が成り立つとは言えない**特異モデル**に属します．
特異モデルでは，**AICやBICによるモデル選択が有効ではない**こともあるので注意が必要です．
**WAIC,WBIC**という情報量規準は，特異モデルでも使用できる基準として提案されました．
気になる方は是非調べてみてください．

{{</notice>}}


### GMM のベイズ推論
さて，本題に入ります．
グラフィカルモデルから，GMMの全変数の同時分布は以下のようになります．

$$
    p(x^n, l^n, \pi^K, \mu^K) = p(x^n, l^n|\pi^K,\mu^k)p(\pi^K)p(\mu^K)
    \tag{5}
$$

与えられるデータは $x^n$ のみです．
このデータ $x^n$ が与えられた際の各パラメータの事後分布は

$$
    p(l^n, \pi^K, \mu^K | x^n) \propto p(x^n, l^n|\pi^K,\mu^k)p(\pi^K)p(\mu^K)
    \tag{6}
$$

となります．
ここで，$p(x^n, l^n|\pi^K,\mu^k)$ が**尤度関数**，$p(\pi^K),p(\mu^K)$ はそれぞれ**事前分布**です．

事前分布を具体的に定めます．
混合比 $\pi^K$ の事前分布は，以下の**ディリクレ（Dirichlet）分布**を仮定します．

$$
    p(\pi^K|\bm\alpha) = \text{Dir}(\pi^K|\bm\alpha)=\frac{1}{\text{B}(\bm\alpha)}\prod_{k=1}^K\pi_k^{\alpha_k - 1},~~~~~\bm\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_K)\in\mathbb R^K
    \tag{7}
$$

$\text{B}(\bm\alpha)$はベータ関数です．
ハイパーパラメータ $\bm\alpha$ は $\mathbb I^K$ とします．

また，平均 $\mu^K$ の事前分布は以下の**ガウス分布**を仮定します．

$$
    p(\mu^K) = \prod_{k=1}^K\mathcal N(\bm\mu_k|0,3I) = \left(6\pi\right)^{-K}\prod_{k=1}^K\exp\left(-\frac{||\bm\mu_k||_2^2}{6}\right)
    \tag{8}
$$

これらを用いると，注目している変数のみを抽出して，事後分布は以下のように書けます．

$$
    p(l^n, \pi^K, \mu^K | x^n) \propto \left\\{\prod_{i=1}^n\prod_{k=1}^K\pi_k^{l_{i,k}}\mathcal N(\bm x_i|\bm\mu_k,\Sigma_k^{-1})^{l_{i,k}}\right\\}\left\\{\prod_{k=1}^K\pi_k^{\alpha_k - 1}\right\\}\left\\{\prod_{k=1}^K\exp\left(-\frac{||\bm\mu_k||_2^2}{6}\right)\right\\}
    \tag{9}
$$

### MCMCを構成する
これから，式(9)の事後分布に従う確率変数からサンプリングを行うためのMCMC（マルコフ連鎖モンテカルロ法）のアルゴリズムを構築します．
今回は以下のように，各変数を順にサンプリングしていく**Gibbs Sampling** を行います．
$$
    \mu^K\sim p(\mu^K|\pi^K,l^n)
    \tag{10}
$$

$$
    \pi^K\sim p(\pi^K|\mu^K,l^n)
    \tag{11}
$$

$$
    l^n\sim p(l^n|\mu^K,\pi^K)
    \tag{12}
$$


##### <mark>各混合の平均の更新則</mark>

平均 $\mu^K$ の更新について考えます．
式(9)の事後分布において，平均 $\mu^K$ に依存する項のみを抽出すると

$$
    p(\mu^K|\pi^K,l^n) \propto \left\\{\prod_{i=1}^n\prod_{k=1}^K\mathcal N(\bm x_i|\bm\mu_k,I)^{l_{i,k}}\right\\}\left\\{\prod_{k=1}^K\exp\left(-\frac{||\bm\mu_k||_2^2}{6}\right)\right\\}
    \tag{13}
$$

となります．
ここで，$k$ 番目の混合の平均 $\bm\mu_k$ の更新を考えます．
式(13)を $\bm\mu_k$ に着目した形で変形すると，詳細は省きますが，

$$
\begin{aligned}
   p(\mu^K|\pi^K,l^n)
   &\propto  \left\\{\prod_{i\in k\text{-th class}}\exp\left(-\frac{||\bm x_i-\bm\mu_k||_2^2}{2}\right)\right\\}\left\\{\exp\left(-\frac{||\bm\mu_k||_2^2}{6}\right)\right\\}\cr
   %&= \exp\left\\{-\frac{\sum_{i\in k\text{-th class}}||\bm x_i-\bm\mu_k||_2^2 + \frac{1}{3}||\bm\mu_k||_2^2}{2}\right\\}\cr
   %&= \exp\left\\{-\frac{1}{2}\left(\sum_{i\in k\text{-th class}}\left\\{\bm x_i^T\bm x_i - 2\bm x_i^T\bm\mu_k + \bm\mu_k^T\bm\mu_k\right\\} + \frac{1}{3}\bm\mu_k^T\bm\mu_k\right)\right\\}\cr
   %& \propto \exp\left\\{-\frac{1}{2}\left(\sum_{i\in k\text{-th class}}\left\\{-2\bm x_i^T\bm\mu_k\right\\} + n_k\bm\mu_k^T\bm\mu_k + \frac{1}{3}\bm\mu_k^T\bm\mu_k\right)\right\\}\cr
   %&= \exp\left\\{-\frac{1}{2}\left(-2\bar{\bm x}_k^T\bm\mu_k + \frac{3n_k + 1}{3}\bm\mu_k^T\bm\mu_k\right)\right\\}\cr
   %&= \exp\left\\{-\frac{3n_k+1}{6}\left(-\frac{6}{3n_k + 1}\bar{\bm x}_k^T\bm\mu_k + \bm\mu_k^T\bm\mu_k\right)\right\\}\cr
   &\propto \exp\left\\{-\frac{3n_k+1}{6}\left|\left|\bm\mu_k - \frac{3}{3n_k + 1}\bar{\bm x}_k\right|\right|_2^2\right\\}\cr
   &\propto \mathcal N\left(\bm\mu_k\Bigg|\frac{3}{3n_k + 1}\bar{\bm x}_k, \frac{3}{3n_k + 1}I\right)
\end{aligned}
\tag{14}
$$

となります．
ここで，$i\in k\text{-th class}$ は，$k$ 番目の混合に属するデータの添字 $i$ の集合を意味し，$\bar{\bm x}_k$ は，$k$ 番目の混合におけるデータの和を表します．
つまり，

$$
    \bar{\bm x}\_k = \sum_{i\in k\text{-th class}}\bm x_i
    \tag{15}
$$

です．
これより，$k$ 番目の混合の平均 $\bm\mu_k$ の更新は**事前分布であるガウス分布の更新**となります．
よって，この更新後のガウス分布からサンプリングを行います．
この操作を $k=1,2,\cdots, K$ ついて行えば良さそうです．

{{<notice note 観察>}}
式(14)の事後分布であるガウス分布の分散・共分散行列にかかる係数が，その混合に属するデータ数に従って小さくなることが観察できます．
このことから，データ数が多い混合ほど，平均の推定精度が上がることが予想できます．

また，式(14)のガウス分布の平均を見てみると，$n_k\rightarrow\infty$ であれば，$\bar{\bm x}_k$ は $k$ 番目に属するデータ点の平均となっていることがわかります．
データ数が少ないと，事前分布の影響である，分母の $+1$ が効いてきそうです．
{{</notice>}}

##### <mark>混合比の更新則</mark>

混合比 $\pi^K$ の更新について考えます．
式(9)の事後分布において，混合比 $\pi^K$ に依存する項のみを抽出すると

$$
    p(\pi^K|\mu^K,l^n) \propto \left\\{\prod_{i=1}^n\prod_{k=1}^K\pi_k^{l_{i,k}}\right\\}\left\\{\prod_{k=1}^K\pi_k^{\alpha_k - 1}\right\\}
    \tag{16}
$$

となります．
これを， $k$ 番目の混合に属するデータ点の数 $n_k$ を用いて変形すると

$$
    p(\pi^K|\mu^K,l^n) \propto \prod_{k=1}^K\pi_k^{n_k-\alpha_k - 1} \propto \text{Dir}(\pi^K|\bm\alpha')
    \tag{17}
$$
となります．
ここで，$\bm\alpha'$ は

$$
    \bm\alpha' = (n_1 - \alpha_1 - 1, n_2 - \alpha_2 - 1, \cdots, n_K - \alpha_K - 1)
    \tag{18}
$$

で，混合比 $\pi^K$ の更新は，**事前分布であるディリクレ分布の更新**となることがわかります．
よって，この更新後のディリクレ分布からのサンプリングを行います．

##### <mark>潜在変数の更新則</mark>

潜在変数 $l^n$ の更新について考えます．
式(9)の事後分布において，潜在変数 $l^n$ に依存する項のみを抽出すると，

$$
    p(l^n|\mu^K,\pi^K) \propto \left\\{\prod_{i=1}^n\prod_{k=1}^K\pi_k^{l_{i,k}}\mathcal N(\bm x_i|\bm\mu_k,\Sigma_k^{-1})^{l_{i,k}}\right\\}
    \tag{19}
$$

となります．
ここでさらに，$i$ 番目の潜在変数のみを抽出すると，

$$
    p(\bm l_i|\mu^K,\pi^K) \propto \left\\{\prod_{k=1}^K\pi_k^{l_{i,k}}\mathcal N(\bm x_i|\bm\mu_k,\Sigma_k^{-1})^{l_{i,k}}\right\\}
    \tag{20}
$$

となります．
式(20)の値は，とりうる $\bm l_i$ の全てについて求めることができます．
（<mark>これは，$i$ 番目のデータの $1$ 番目の混合，$2$ 番目の混合，$\dots$，$K$ 番目の混合への所属確率を計算していることになります．</mark>）
そうすると，正規化を行うことで，$\bm l_i$ の事後確率が出来上がります．
そこからサンプリングを行います．


### 数値実験
冒頭で示した2次元データを再掲します．
このデータは，混合比を

$$
    \pi_1 = 0.2, \pi_2 = 0.3, \pi_3 = 0.5
$$
とし，各混合における平均値は

$$
    \bm\mu_1 = (-2, -2), \bm\mu_2 = (0, 2), \bm\mu_3 = (-2, 2)
$$

としてデータを発生させたものです．


### まとめ