---
title: LASSO と僕は同い年：① 線形回帰
date: 2022-01-28
categories: [Research-ish, Programming]
tags: [Machine Learning, Mathematical Optimization]
image: "/image/blog-pic.jpg"
math: true
---

本記事では，LASSOへの足掛かりとして，線形回帰モデルを紹介します．
線形回帰モデルはとてもシンプルなモデルであり

### 直線から始める線形回帰モデル
まずは簡単な例で直感的な理解を深めてみましょう．
例えば，図のようなデータが与えられたとします．

中学生で学んだ$1$次関数を復習します．
入力変数を $x\in\mathbb R^1$, 出力変数を $y\in\mathbb R^1$ とします．
$1$次関数は傾き $a\in\mathbb R^1$ と切片 $b\in\mathbb R^1$ を用いて，

$$
    y = ax + b
$$

で表されます．
線形回帰モデルとは，出力変数を入力変数の線形な関数で回帰するものです．
今，$n$ 個の出力変数を$1$次元であるとし， 

$$
y^n=(y_1,y_2,\cdots,y_n),~~y_i\in\mathbb R^1~(i=1,2,\cdots,n)
$$

と書きます．
また，$n$ 個の入力変数を $d$ 次元であるとします．
後々のために，入力変数の$1$次元目には定数 $1$を持たせておき，$d+1$次元のベクトルとして扱います．

$$
x^n=(\bm x_1,\cdots,\bm x_n),~~\bm x_i=(1,x_{i1},x_{i2},\cdots,x_{id})\in\mathbb R^{d+1}~(i=1,2,\cdots,n)
$$

ここで，回帰係数 $\bm w$ を

$$
\bm w = (w_{0},w_{1},w_{2}\cdots,w_{d})\in\mathbb R^{d+1}
$$

とおき，入力変数と出力変数の関係が

$$
\bm y_i = \bm w^T\bm x + \epsilon,~(i=1,2,\cdots,n)
$$

であることを仮定するモデルを**線形回帰モデル**と呼びます．
ここで，$\epsilon\in\mathbb R^1$ は出力に加重されるノイズ，もしくは誤差です．
線形回帰モデルのパラメータは $\bm w$ ですが，$n$ 個のデータ $(x^n,y^n)$ が与えられた際に，全ての入出力関係を表現できる回帰係数となることが理想です．


### 入力変数を多次元に拡張

### 出力変数を多次元へ拡張

### 一般化線形回帰モデル


