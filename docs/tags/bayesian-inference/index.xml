<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian Inference on TH&#39;s Blog</title>
    <link>https://t0m0ya1997.github.io/whoami/tags/bayesian-inference/</link>
    <description>Recent content in Bayesian Inference on TH&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>jp</language>
    <lastBuildDate>Mon, 24 Jan 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://t0m0ya1997.github.io/whoami/tags/bayesian-inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>XorShiftという乱数生成法が早いらしい。</title>
      <link>https://t0m0ya1997.github.io/whoami/posts/xorshiro/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://t0m0ya1997.github.io/whoami/posts/xorshiro/</guid>
      <description>参考文献 </description>
    </item>
    
    <item>
      <title>メトロポリス法ってなんだし。</title>
      <link>https://t0m0ya1997.github.io/whoami/posts/metropolis_hastings/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://t0m0ya1997.github.io/whoami/posts/metropolis_hastings/</guid>
      <description>本記事では，マルコフ連鎖モンテカルロ法（Markov Chain Monte Carlo: MCMC）を実現する手法の１つである，メトロポリス法（Metropolis-Hastings Method）について解説してみます． また，簡単 of 簡単な数値実験を行います．
メトロポリス法(Metropolis-Hastings Method) 数値実験 今回はメトロポリス法による1次元標準正規分布からのサンプリングを行います． 確率変数を $x\in\mathbb R$ とします． 標準正規分布の確率密度関数 $p(x)$ は
$$ p(x|\mu = 0, \sigma^2 = 1) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}x^2\right), $$
となります． 現時点でのサンプルを $x$ とし，確率的摂動 $\delta\in\mathbb R$ を加えた，遷移先の候補点を $x&#39; = x + \delta$とします． メトロポリス法の更新則として，$x,x&#39;$ を用いて</description>
    </item>
    
    <item>
      <title>ベイズから眺めるMAP推定は豊洲の高層ビルから眺める東京湾よりも美しいのか。</title>
      <link>https://t0m0ya1997.github.io/whoami/posts/bayes_map/</link>
      <pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://t0m0ya1997.github.io/whoami/posts/bayes_map/</guid>
      <description>本記事では，ベイズの枠組みでLASSO回帰やRidge回帰に代表される，正則化付き最小二乗法を眺めてみます． 正則化は機械学習を学ぶと早い段階で出てくる，過学習を抑制するためのテクニックですよね． この正則化も実はベイズの枠組みで見ると，MAP（事後確率最大化：Maximum a posteriori）推定に相当します． 「当たり前じゃん」と思った方には不毛な記事になりますが，是非最後まで読んでいただけると幸いです．
1. ベイズ推論（Bayesian Inference） ベイズ推論については渡辺先生のこちらが詳しいです． 与えられたデータ $x^n=(x_1,\cdots,x_n),~ x_i\in\mathbb R^N$ は真の確率分布$q(x)$にi.i.dに従う確率変数 $X_i\in\mathbb R^N,~i=1,\cdots,n$ の実現値と考えます． つまり，データの同時分布は $$ q(x^n)=\prod_{i=1}^nq(x_i) =q(x_1)q(x_2)\cdots q(x_n), $$ となります． ここで，真の分布 $q(x)$ は知ることのできないものとして考えます． ベイズ推論とは，真の分布 $q(x)$ を与えられたデータ $x^n$ から推測することを指します．
それでは，具合的にどうやって推測を行うのでしょうか？ ここで，人間はパラメータ $w\in W\subset \mathbb R^d$ によって形状が決定する， $x\in\mathbb R^N$ 上の条件付き確率分布 $p(x|w)$ (確率モデル) と $w\in W$ 上の確率分布 $\varphi(w)$ (事前分布) を用意します． 渡辺先生の本では，逆温度 $\beta$ での事後確率が登場しますが，本記事では，データ $x^n$ が与えられたときの，パラメータ $w$ の事後確率 $p(w|X^n)$ を一般的なベイズ統計の教科書に合わせて以下で定義します．</description>
    </item>
    
  </channel>
</rss>
